{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "S4_1_Dimensionality.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0-WlA6efBRki"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yasminghd/2022_ML_Earth_Env_Sci/blob/main/Lab_Notebooks/S4_1_Dimensionality.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Chapter 8 â€“ Dimensionality Reduction**"
      ],
      "metadata": {
        "id": "fab2zKXwAinB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src='https://unils-my.sharepoint.com/:i:/g/personal/tom_beucler_unil_ch/EX7KlNGWYypLnH_53OnJR6oBjfgb_gCZ4gmnOeR68a6zMA?download=1'>\n",
        "<center> Caption: <i>Denise diagnoses an overheated CPU at our data center in The Dalles, Oregon. <br> For more than a decade, we have built some of the world's most efficient servers.</i> <br> Photo from the <a href='https://www.google.com/about/datacenters/gallery/'>Google Data Center gallery</a> </center>"
      ],
      "metadata": {
        "id": "y7Q5WigQxsVd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Our world is increasingly filled with data from all sorts of sources, including environmental data. Can we reduce the data to a reduced, meaningful space to save on computation time and increase explainability?*"
      ],
      "metadata": {
        "id": "XGGHmOj1ygXe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook will be used in the lab session for week 4 of the course, covers Chapters 8 of GÃ©ron, and builds on the [notebooks made available on _Github_](https://github.com/ageron/handson-ml2).\n",
        "\n",
        "Need a reminder of last week's labs? Click [_here_](https://colab.research.google.com/github/tbeucler/2022_ML_Earth_Env_Sci/blob/main/Lab_Notebooks/Week_3_Decision_Trees_Random_Forests_SVMs.ipynb) to go to notebook for week 3 of the course."
      ],
      "metadata": {
        "id": "AlTDG-57-aAI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Setup\n",
        "\n",
        "First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn â‰¥0.20."
      ],
      "metadata": {
        "id": "0-WlA6efBRki"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zw6fcA3O-Uls"
      },
      "outputs": [],
      "source": [
        "# Python â‰¥3.5 is required\n",
        "import sys\n",
        "assert sys.version_info >= (3, 5)\n",
        "\n",
        "# Scikit-Learn â‰¥0.20 is required\n",
        "import sklearn\n",
        "assert sklearn.__version__ >= \"0.20\"\n",
        "\n",
        "# Common imports\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "# to make this notebook's output stable across runs\n",
        "rnd_seed = 42\n",
        "rnd_gen = np.random.default_rng(rnd_seed)\n",
        "\n",
        "# To plot pretty figures\n",
        "%matplotlib inline\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "mpl.rc('axes', labelsize=14)\n",
        "mpl.rc('xtick', labelsize=12)\n",
        "mpl.rc('ytick', labelsize=12)\n",
        "\n",
        "# Where to save the figures\n",
        "PROJECT_ROOT_DIR = \".\"\n",
        "CHAPTER_ID = \"dim_reduction\"\n",
        "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\", CHAPTER_ID)\n",
        "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
        "\n",
        "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
        "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
        "    print(\"Saving figure\", fig_id)\n",
        "    if tight_layout:\n",
        "        plt.tight_layout()\n",
        "    plt.savefig(path, format=fig_extension, dpi=resolution)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dimensionality Reduction using PCA\n",
        "\n",
        "This week we'll be looking at how to reduce the dimensionality of a large dataset in order to improve our classifying algorithm's performance! With that in mind, let's being the exercise by loading the MNIST dataset.\n",
        "\n",
        "###**Q1) Load the input features and truth variable into X and y, then split the data into a training and test dataset using scikit's train_test_split method. Use *test_size=0.15*, and remember to set the random state to *rnd_seed!***\n",
        "\n",
        "*Hint 1: The `'data'` and `'target'` keys for mnist will return X and y.*\n",
        "\n",
        "*Hint 2: [Here's the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for train/test split.*"
      ],
      "metadata": {
        "id": "H3QU33M3D--N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the mnist dataset\n",
        "from sklearn.datasets import fetch_openml\n",
        "mnist = fetch_openml('mnist_784', version=1, as_frame=False)"
      ],
      "metadata": {
        "id": "H9slNfR3D-kg"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#mnist"
      ],
      "metadata": {
        "id": "8pF7uxoshurg"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load X and y\n",
        "X = mnist.data\n",
        "y = mnist.target"
      ],
      "metadata": {
        "id": "zNcNkJ3u92cW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the train/test split function from sklearn\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "metadata": {
        "id": "yOmYNwuT920P"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,test_size = 0.15, random_state=rnd_seed)"
      ],
      "metadata": {
        "id": "N9nYrB_v98vr"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now once again have a training and testing dataset with which to work with. Let's try training a random forest tree classifier on it. You've had experience with them before, so let's have you import the `RandomForestClassifier` from sklearn and instantiate it.\n",
        "\n",
        "###**Q2) Import the `RandomForestClassifier` model from sklearn. Then, instantiate it with 100 estimators and set the random state to `*rnd_seed!*`**\n",
        "\n",
        "*Hint 1: [Here's the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for `RandomForestClassifier`*\n",
        "\n",
        "*Hint 2: [Here's the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) for train/test split.*\n",
        "\n",
        "*Hint 3: If you're still confused about **instantiation**, there's a [blurb on wikipedia](https://en.wikipedia.org/wiki/Instance_(computer_science)) describing it in the context of computer science.*"
      ],
      "metadata": {
        "id": "EhBQOdVxfr2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code\n",
        "from sklearn.ensemble import RandomForestClassifier"
      ],
      "metadata": {
        "id": "ZZaWwNGUg9Qb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rnd_clf = RandomForestClassifier(n_estimators=100, #Number of estimators \n",
        "                 random_state=rnd_seed) #Random State"
      ],
      "metadata": {
        "id": "qJc0deCO-Ibt"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're now going to measure how quickly the algorithm is fitted to the mnist dataset! To do this, we'll have to import the `time` library. With it, we'll be able to get a timestamp immediately before and after we fit the algorithm, and we'll get the time by calculating the difference.\n",
        "\n",
        "###**Q3) Import the time library and calculate how long it takes to fit the `RandomForestClassifier` model.**\n",
        "\n",
        "*Hint 1: [Here's the documentation](https://docs.python.org/3/library/time.html#time.time) to the function used for getting timestamps*\n",
        "\n",
        "*Hint 2: [Here's the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.fit) for the fitting method used in `RandomForestClassifier`.*"
      ],
      "metadata": {
        "id": "gi1HTS-KjUJ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time"
      ],
      "metadata": {
        "id": "EZaQPn2XkV06"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t0 = time.time() # Load the timestamp before running\n",
        "rnd_clf.fit(X_train, y_train) # Fit the model with the training data\n",
        "t1 = time.time()  # Load the timestamp after running"
      ],
      "metadata": {
        "id": "B4jPNCXl-OIM"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_t_rf = t1-t0\n",
        "\n",
        "print(f\"Training took {train_t_rf:.2f}s\")"
      ],
      "metadata": {
        "id": "LFuLLVWj-PXZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "038f32b3-5d17-454f-b68e-83b10a620290"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training took 36.53s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We care about more than just how long we took to trian the model, however! Let's get an accuracy score for our model.\n",
        "\n",
        "###**Q4) Get an accuracy score for the predictions from the RandomForestClassifier**\n",
        "\n",
        "*Hint 1: [Here is the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) for the `accuracy_score` metric in sklearn.* \n",
        "\n",
        "*Hint 2: [Here is the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html#sklearn.ensemble.RandomForestClassifier.predict) for the predict method in `RandomForestClassifier`*"
      ],
      "metadata": {
        "id": "X0-hEhlOnLqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score # Import the accuracy score metric"
      ],
      "metadata": {
        "id": "lscBW_sFnLVS"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a set of predictions from the random forest classifier\n",
        "y_pred = rnd_clf.predict(X_test)   # Get a set of predictions from the test set"
      ],
      "metadata": {
        "id": "x-93C_-n-cle"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf_accuracy = accuracy_score(y_test, y_pred)  # Feed in the truth and predictions"
      ],
      "metadata": {
        "id": "n09PnHuy-cTf"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"RF Model Accuracy: {rf_accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "BDjIvrLC-hc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cbb532f8-fa1d-42e4-d009-9368f011f486"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF Model Accuracy: 96.71%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's try doing the same with with a logistic regression algorithm to see how it compares. \n",
        "\n",
        "###**Q5) Repeat Q2-4 with a logistic regression algorithm using sklearn's `LogisticRegression` class. Hyperparameters: `multi_class='multinomial'` and `solver='lbfgs'`**\n",
        "\n",
        "*Hint 1: [Here is the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for the `LogisticRegression` class."
      ],
      "metadata": {
        "id": "XEZX7xBAHJj9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "kwX8ZwzQI6p6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_clf = LogisticRegression(multi_class=\"multinomial\", #Multiclass\n",
        "                solver=\"lbfgs\",  #Solver\n",
        "                random_state=42) #Random State"
      ],
      "metadata": {
        "id": "CvUwrxtS-mTf"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "t0 = time.time() # Timestamp before training\n",
        "log_clf.fit(X_train, y_train) # Fit the model with the training data\n",
        "t1 = time.time() # Timestamp after training"
      ],
      "metadata": {
        "id": "F6Dr9j1T-mgz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbeb3d90-13fa-4b67-d048-2c192907dfaf"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_t_log = t1-t0\n",
        "print(f\"Training took {train_t_log:.2f}s\")"
      ],
      "metadata": {
        "id": "9WexZJ7n-mt6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2e34b05-5c1a-493d-f3a6-36334af05f59"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training took 26.17s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a set of predictions from the logistric regression classifier\n",
        "y_pred = log_clf.predict(X_test)   # Get a set of predictions from the test set\n",
        "log_accuracy = accuracy_score(y_test, y_pred)  # Feed in the truth and predictions"
      ],
      "metadata": {
        "id": "Armw_a0V-mAs"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Log Model Accuracy: {log_accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "guKqI9Um-_zv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f1bb471-ad5f-4c62-bea4-8b0404276417"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log Model Accuracy: 92.05%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Up to now, everything that we've done are things we've done in previous labs - but now we'll get to try out some algorithms useful for reducing dimensionality! Let's use principal component analysis. Here, we'll reduce the space using enough axes to explain over 95% of the variability in the data...\n",
        "\n",
        "###**Q6) Import scikit's implementation of `PCA` and fit it to the training dataset so that 95% of the variability is explained.**\n",
        "\n",
        "*Hint 1: [Here is the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) for scikit's `PCA` class.*\n",
        "\n",
        "*Hint 2: [Here is the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html#sklearn.decomposition.PCA.fit_transform) for scikit's `.fit_transform()` method.*"
      ],
      "metadata": {
        "id": "b_5XiaQfJ5NV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA # Importing PCA"
      ],
      "metadata": {
        "id": "rrP5043rJc-1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pca = PCA(n_components=0.95) # Set number of components to explain 95% of variability"
      ],
      "metadata": {
        "id": "UZAeoAlI_Ok9"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_reduced = pca.fit_transform(X_train) # Fit-transform the training data"
      ],
      "metadata": {
        "id": "b3FHiYMA_OwR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_reduced = pca.transform(X_test) # Transform the test data (!!No fitting!!)"
      ],
      "metadata": {
        "id": "zydXZOAV_T1U"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q7) Repeat Q3 & Q4 using the *reduced* `X_train` dataset instead of `X_train`.**"
      ],
      "metadata": {
        "id": "mKXeXWn4M8K1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete the code\n",
        "\n",
        "t0 = time.time() # Load the timestamp before running\n",
        "rnd_clf.fit(X_train_reduced, y_train) # Fit the model with the reduced training data\n",
        "t1 = time.time()  # Load the timestamp after running"
      ],
      "metadata": {
        "id": "m1oZFFfljH0N"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_t_rf = t1-t0\n",
        "\n",
        "print(f\"Training took {train_t_rf:.2f}s\")"
      ],
      "metadata": {
        "id": "db8zIrD4_Xa4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e711b7e-0fa0-4e42-f9b7-7369abbdabcb"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training took 108.13s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a set of predictions from the random forest classifier\n",
        "y_pred = rnd_clf.predict(X_test_reduced)  # Get predictions from the reduced test set"
      ],
      "metadata": {
        "id": "jNisAXlgnUMe"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "red_rf_accuracy = accuracy_score(y_test, y_pred)  # Feed in the truth and predictions\n",
        "\n",
        "print(f\"RF Model Accuracy on reduced dataset: {red_rf_accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "S-umJB9I_dnc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "651e33d4-249a-44d6-80dc-b287445d0a6f"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RF Model Accuracy on reduced dataset: 94.72%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###**Q8) Repeat Q5 using the *reduced* X_train dataset instead of X_train.**"
      ],
      "metadata": {
        "id": "46j-guE8NStk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Complete the code\n",
        "\n",
        "t0 = time.time() # Timestamp before training\n",
        "log_clf.fit(X_train_reduced, y_train) # Fit the model with the reduced training data\n",
        "t1 = time.time() # Timestamp after training"
      ],
      "metadata": {
        "id": "JerFiDoKMpAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2102f8d0-b79e-48a9-b9f3-0d9e6d36dd6b"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_t_log = t1-t0\n",
        "print(f\"Training took {train_t_log:.2f}s\")"
      ],
      "metadata": {
        "id": "efar-d1W_fuu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76143e8b-7816-4445-9218-f9085c1d5cab"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training took 10.46s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a set of predictions from the logistric regression classifier\n",
        "y_pred = log_clf.predict(X_test_reduced)   # Get a set of predictions from the test set"
      ],
      "metadata": {
        "id": "R3Pc9LRK_f4I"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "log_accuracy = accuracy_score(y_test, y_pred)  # Feed in the truth and predictions\n",
        "print(f\"Log Model Accuracy on reduced training data: {log_accuracy:.2%}\")"
      ],
      "metadata": {
        "id": "cG2Jxz1g_gCm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "669451ad-7866-43ed-af28-9ed2eb3351fd"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Log Model Accuracy on reduced training data: 91.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now compare how well the random forest classifier and logistic regression classifier performed on both the full dataset and the reduced dataset. What were you able to observe? "
      ],
      "metadata": {
        "id": "_P_-tnZstz99"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write your comments on the performance of the algorithms in this box, if you'd like ðŸ˜€\n",
        "(Double click to activate editing mode)\n",
        "What I observed is that using the principal components slightly reduces the accuracy, which is logical as using them leaves part of the variance unexplained.\n",
        "However, it is expected that using the PCs will decrease the processig time (not considerig the fitting and transforming phases), which is confirmed in using logistic regression, but interestingly using PCA with random forests has increased the processing time as well!"
      ],
      "metadata": {
        "id": "6AFlS89UuZTy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "dIbEmQha7JWV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}